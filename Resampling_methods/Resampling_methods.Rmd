---
title: "Resampling_methods"
author: "Hani Nabulsi"
date: "24/11/2020"
output:
  html_document:
    keep_md: true
---
```{r global_options, include = TRUE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE,fig.path = "README_figs/README-" ) 
```

The Validation Set Approach

We explore the use of the validation set approach in order to estimate the test error rate that result from fitting various linear models on Auto data set.

```{r}
library(ISLR)
set.seed(1) #To reproduce the randomness in same way more times
train= sample(392,196) #We take randomly 196 obs from a dataset with 392 obs (1:392)
```
```{r}
lm.fit=lm(mpg~horsepower,data=Auto,subset=train) #we fit the linear regr model on training set. Remember that subset takes an array with indexes
```
```{r}
prediction_test = predict(lm.fit,Auto[-train,]) #Prediction on complementary of Training set--> Test sets 
mpg_test = Auto$mpg[-train]  #Values of responce of test set
mean((mpg_test-prediction_test)^2) #Test error rate (MSE)
```
Now we calculate the Test Error Rate on with a quadratic and cube polynomio of the features.

```{r}
lm.fit2 = lm(mpg~poly(horsepower,2),data=Auto,subset=train)
prediction_test_2 = predict(lm.fit2,Auto[-train,])
mean((mpg_test-prediction_test_2)^2)
```

```{r}
lm.fit3 = lm(mpg~poly(horsepower,3),data=Auto,subset=train)
prediction_test_3 = predict(lm.fit3,Auto[-train,])
mean((mpg_test-prediction_test_3)^2)
```

Now we calculate the Test Error Rate with another random splitting in Training Set/Test Set

```{r}
#For linear Regression
set.seed(2)
train= sample(392,196)

lm.fit=lm(mpg~horsepower,data=Auto,subset=train) 
prediction_test = predict(lm.fit,Auto[-train,])
mpg_test = Auto$mpg[-train]  
mean((mpg_test-prediction_test)^2) 
```
```{r}
#For quadratic polynomio
lm.fit2 = lm(mpg~poly(horsepower,2),data=Auto,subset=train)
prediction_test_2 = predict(lm.fit2,Auto[-train,])
mean((mpg_test-prediction_test_2)^2)
```

```{r}
#For cubic polynomio
lm.fit3 = lm(mpg~poly(horsepower,3),data=Auto,subset=train)
prediction_test_3 = predict(lm.fit3,Auto[-train,])
mean((mpg_test-prediction_test_3)^2)
```


If we change the random splitting in Training Set and Test Set and recalculate the Test Error Rate with the different models we can see that quadratic model is better than linear model!!

MOST IMPORTANT: if we change the random splitting in Training Set/Test Set, there is a variability in Test Error Rate among the models!

Leave-One-Out Cross-Validation

```{r}
library(boot) # to load cv.glm()
glm.fit = glm(mpg~horsepower,data=Auto) #if we don't explicit family= binomial, we have a linear regression. We need glm() because so we can use cv.glm
cv.err = cv.glm(Auto,glm.fit)
cv.err$delta #delta-->cross-validation results. We have the two value equal (we see a situation where they are different). It represents the mean of all test errors on a single row.
```

We use LOOCV with more type of polynomial regression

```{r}
cv.error =rep(0,5)
for (i in 1:5){ #We use for loop for fit many polynomial model at the same time
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1] #We memorize the vector of all means of all test errors.
}
```
```{r}
cv.error  #We estimate the test MSE. We can see the improve from linear to quadratic is relevant. It's no so relevant with high-order polynomials.
```

k_Fold Cross-Validation

```{r}
set.seed(17) #We need to split randomly the dataset in 10 subsets
cv.error.10 = rep(0,10)
for (i in 1:10){ #We use for loop for fit many polynomial model at the same time
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1] #We memorize the vector of all means of all test errors.  cv.glm$delta[1]=test MSE, cv.glm$delta[2] = bias corrected version of first one. It's very similar to first one.
}
```
```{r}
cv.error.10  #As before, from linear to quadratic fit there is a good improvement in test MSE. Not so good improvement with high-order polynomials.
```

THE BOOTSTRAP

We use bootstrap to estimate the accuracy of a predictive model ad can be applied almost in all situations.

We perform bootstrap analysis in 2 steps:
-Create a function that computes the statistic of interest (alpha.fn)
-Use the boot() function to perform the bootstrap by repeatedly sampling obs from the dataset with  replacement.

We use Portfolio dataset (same example in theory).

```{r}
alpha.fn=function(data,index){ #as inpute the dataset and the index of the obs to use to estimate alpha
X=data$X[index]
Y=data$Y[index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
```
```{r}
#We try the function
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T)) #we choose the indexes by taking randomly 100 values,with repetitions, from a range 1-100
```

We use boot() function to perform bootstrap analysis: we calculate alpha n times and calculate the standard deviation.

```{r}
library(boot)
boot(Portfolio,alpha.fn,R=1000)#we calculate alpha 1000 times by taking 1000 times samples with repetition from Portfolio dataset.
```

Where original= estimation of alpha (alpha^), std.error is the std.err(alpha^).

Estimating the accuracy of the linear Regression Model:

We can use Bootstrap  to calculate the variability of the coefficients/predictions for a statistical learning method.

```{r}
boot.fn=function(data,index) #the function calculate the betas of a simple linear regr
return(coef(lm(mpg~horsepower,data=data,subset=index))) #we don't use {} because there is a single line
```
```{r}
set.seed(1)
boot.fn(Auto,sample(392,392,replace=T)) #As indexes we take randomly all obs in the Auto dataset with repetation.
boot.fn(Auto,sample(392,392,replace=T))
```

As we can see, the parameters change each time (because we choose indexes randomly).

Now we use boot() function to compute std.err of 1000 bootstrap and estimated values of betas

```{r}
boot(Auto,boot.fn,1000)
```

Now we calculate SE(B0^) and SE(B1^) with statistical software

```{r}
summary(lm(mpg~horsepower,data=Auto))$coef
```


The bootstrap std.err is more accurate with Bootstrap method.

Now we calculate the std.error for betas for quadratic polynomio.

```{r}
boot.fn_2=function(data,index) 
return(coef(lm(mpg~poly(horsepower,2),data=data,subset=index))) 
set.seed(1)
boot(Auto,boot.fn_2,1000)
```

Now we calculate std.error with R package...

```{r}
summary(lm(mpg~poly(horsepower,2),data=Auto))$coef
```

We can see that betas std.err of Bootstrap estimation and R package are very similar. This condition is present because the quadratic model fit very well the data (relation between X and Y).